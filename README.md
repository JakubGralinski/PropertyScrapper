# ğŸŒŸâœ¨ Deep Seek Crawler & Warsaw Housing Data Analysis ğŸš€ğŸ™ï¸

Welcome to **Property Scrapper** â€“ your one-stop solution for **scraping** and **analyzing** housing/property data in Warsaw! This project combines advanced web crawling techniques with state-of-the-art data processing and machine learning to uncover trends in the housing market. ğŸ’¡ğŸ“Š

---

## ğŸ“‚ Project Structure

.
â”œâ”€â”€ scrapper.ipynb              # Notebook for web scraping with Selenium & async crawling ğŸ¤–ğŸŒ
â”œâ”€â”€ properties.ipynb            # Notebook for data processing, ML modeling, & visualization ğŸ“ˆğŸ¨
â”œâ”€â”€ config.py                   # Configuration constants (URLs, CSS selectors, etc.) âš™ï¸
â”œâ”€â”€ models
â”‚   â””â”€â”€ property.py             # Pydantic data model for a Property ğŸ ğŸ”
â”œâ”€â”€ utils
â”‚   â”œâ”€â”€ data_utils.py           # Utility functions for processing & saving data ğŸ’¾ğŸ› ï¸
â”‚   â””â”€â”€ scraper_utils.py        # Helper functions for web scraping (driver init, scrolling, etc.) ğŸ”§
â”œâ”€â”€ requirements.txt            # Python package dependencies ğŸ“¦
â”œâ”€â”€ .env                        # Environment variables (e.g., GROQ_API_KEY) ğŸ”‘
â”œâ”€â”€ .gitignore                  # Files & folders to ignore (e.g., .env, CSV outputs) ğŸš«
â””â”€â”€ README.md                   # This awesome README! ğŸ“–ğŸ‰

---

## ğŸš€ Installation

### ğŸ”§ Prerequisites

- **Python 3.12** (or a compatible version) ğŸ
- **Conda** (highly recommended for environment management) ğŸŒ±

### ğŸ“¥ Setup Steps

1. **Clone the Repository:**

   ```bash
   git clone https://github.com/yourusername/deep-seek-crawler.git
   cd deep-seek-crawler

	2.	Create and Activate a Conda Environment:

conda create -n deep-seek-crawler python=3.12 -y
conda activate deep-seek-crawler


	3.	Install Dependencies:

pip install -r requirements.txt


	4.	Configure Environment Variables:
Create a .env file in the project root with content like:

GROQ_API_KEY=your_groq_api_key_here

(Your API key is kept secret â€“ ğŸ”’ no worries!)

ğŸ› ï¸ Usage

1. Web Scraping (scrapper.ipynb) ğŸ¤–ğŸŒ

The web scraper leverages Selenium and BeautifulSoup to:
	â€¢	Navigate to the Otodom.pl listings page for Warsaw ğŸ™ï¸
	â€¢	Dismiss cookie banners ğŸª and scroll to load lazy content â³
	â€¢	Extract property details (title, price, location, URL) using CSS selectors ğŸ¯
	â€¢	Save the extracted data into a CSV file (e.g., listings.csv) ğŸ’¾

To Run the Scraper:
	â€¢	Open scrapper.ipynb in Jupyter Notebook or JupyterLab.
	â€¢	Run all cells (or execute the scrape_otodom() function).
	â€¢	Check your CSV file for the scraped listings! ğŸ‰

2. Housing Data Analysis & Modeling (properties.ipynb) ğŸ“ˆğŸ¨

This notebook handles:
	â€¢	Data Loading & Cleaning:
Loads housing_warsaw_with_coordinates.csv, cleans price fields (removing symbols, commas, etc.), converts data types, and extracts valid district information. ğŸ§¹âœ¨
	â€¢	Exploratory Data Analysis:
Creates histograms, scatter plots, correlation matrices, and even maps using GeoPandas/Contextily (static) and Folium (interactive)! ğŸ“ŠğŸ—ºï¸
	â€¢	Preprocessing Pipeline:
Combines:
	â€¢	A numeric pipeline (missing value imputation, optional feature creation via CombinedAttributesAdder, and scaling) ğŸ”¢
	â€¢	A categorical pipeline (missing value imputation and one-hot encoding for District) ğŸ¨
All merged using a ColumnTransformer to create a final feature array.
	â€¢	Model Training & Evaluation:
Trains several models (Linear Regression, Decision Tree, Random Forest, XGBoost) with hyperparameter tuning (using GridSearchCV) and cross-validation (using RMSE as the metric) to predict Price per mÂ². ğŸ”ğŸ¤–

To Run the Analysis & Modeling:
	â€¢	Open properties.ipynb in Jupyter Notebook or JupyterLab.
	â€¢	Run cells sequentially to:
	â€¢	Load and preprocess the data.
	â€¢	Train and evaluate different models.
	â€¢	Visualize your results with vibrant maps and charts!

âš™ï¸ Configuration
	â€¢	config.py:
Contains key settings such as:
	â€¢	BASE_URL â€“ The starting URL for property listings ğŸŒ
	â€¢	LISTINGS_CONTAINER_CLASS & LISTING_CLASS â€“ CSS selectors for identifying listings on the page ğŸ·ï¸
	â€¢	OUTPUT_FILE â€“ The filename for saving scraped data ğŸ’¾
	â€¢	Scraper Utilities:
Functions in utils/scraper_utils.py handle:
	â€¢	Driver Initialization (with stealth options to bypass bot detection) ğŸš€
	â€¢	Cookie Banner Dismissal ğŸª
	â€¢	Scrolling & Extraction of listings ğŸ”

ğŸ” Advanced Topics
	â€¢	Hyperparameter Tuning:
Use GridSearchCV to fine-tune models like Random Forest and XGBoost. Finer grids and possible target transformations (e.g., log transformation) help reduce error variance. ğŸ“ˆğŸ”¬
	â€¢	Mapping & Visualization:
Create:
	â€¢	Static Maps: Using GeoPandas and Contextily for publication-quality visualizations. ğŸ–¼ï¸
	â€¢	Interactive Maps: Using Folium and branca.colormap for dynamic exploration of housing data. ğŸŒŸ
	â€¢	LLM-Based Extraction (Optional):
Extend the crawler to use a language model extraction strategy (see models/property.py) to transform raw HTML into structured data automatically. ğŸ¤–ğŸ’¬


## ğŸ“š Libraries Used

- **Selenium** & **BeautifulSoup** ğŸ¤–  
  *Purpose:* Used for web scrapingâ€”Selenium drives a real browser to handle dynamic content and JavaScript, while BeautifulSoup parses the HTML to extract data.

- **Crawl4AI** ğŸš€  
  *Purpose:* Enables asynchronous web crawling to efficiently fetch and process multiple pages concurrently.

- **GeoPandas** & **Contextily** ğŸ—ºï¸  
  *Purpose:* GeoPandas makes it easy to work with geospatial data in Python, and Contextily is used to add basemaps (e.g., real-world maps) to static visualizations.

- **Folium** & **branca** ğŸŒ  
  *Purpose:* Folium creates interactive maps for visualizing geospatial data in a web-friendly format, and branca provides color mapping for these visualizations.

- **scikit-learn** ğŸ“ˆ  
  *Purpose:* Provides tools for data preprocessing (e.g., imputation, scaling, encoding), model building (Linear Regression, Decision Trees, Random Forests), and evaluation (cross-validation, GridSearchCV).

- **XGBoost** ğŸ”¥  
  *Purpose:* Implements gradient boosting algorithms for building powerful predictive models, often outperforming traditional ensemble methods on tabular data.

- **Pydantic** ğŸ’»  
  *Purpose:* Used for defining data models with validation (e.g., the Property model) to ensure consistency in the extracted data.

- **requests** ğŸŒ  
  *Purpose:* Facilitates making HTTP requests to fetch web pages when not using a browser-based approach.

- **asyncio** â±ï¸  
  *Purpose:* Powers asynchronous programming, allowing for concurrent web crawling and processing of multiple pages.

- **python-dotenv** ğŸ”‘  
  *Purpose:* Loads environment variables from a `.env` file (e.g., API keys) to keep sensitive information secure.

- **webdriver_manager** ğŸš—  
  *Purpose:* Automatically manages the installation and setup of web drivers for Selenium, simplifying browser automation setup.
